{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\" importing datasets\"\"\"\nimport nltk\nnltk.download('gutenberg')\n\nfrom nltk.corpus import gutenberg\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\n# Load the dataset\ndata = gutenberg.raw('shakespeare-hamlet.txt')\n\n# Save to file\nwith open(\"hamlet.txt\", \"w\", encoding=\"utf-8\") as file:\n    file.write(data)\n\n# Split text into lines\nlines = data.splitlines()\n\nprint(\"----- FIRST 10 LINES -----\")\nfor line in lines[:20]:\n    print(line)\n\nprint(\"\\n----- LAST 10 LINES -----\")\nfor line in lines[-20:]:\n    print(line)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-13T14:08:52.559140Z","iopub.execute_input":"2026-01-13T14:08:52.560124Z","iopub.status.idle":"2026-01-13T14:08:52.567592Z","shell.execute_reply.started":"2026-01-13T14:08:52.560090Z","shell.execute_reply":"2026-01-13T14:08:52.566832Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open('hamlet.txt', 'r') as file:\n    text = file.read().lower()\n\n# Tokenize the text\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts([text])\ntotal_words = len(tokenizer.word_index) + 1\nprint(f\"Total Words: {total_words}\")\n\n# Generate input sequences","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T14:09:07.969458Z","iopub.execute_input":"2026-01-13T14:09:07.970210Z","iopub.status.idle":"2026-01-13T14:09:07.991751Z","shell.execute_reply.started":"2026-01-13T14:09:07.970184Z","shell.execute_reply":"2026-01-13T14:09:07.991081Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input_sequences = []\nfor line in text.split('\\n'):\n    if line.strip():  # Ignore empty lines\n        token_list = tokenizer.texts_to_sequences([line])[0]\n        for i in range(1, len(token_list)):\n            n_gram_sequence = token_list[:i+1]\n            input_sequences.append(n_gram_sequence)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T14:09:08.791940Z","iopub.execute_input":"2026-01-13T14:09:08.792227Z","iopub.status.idle":"2026-01-13T14:09:08.839363Z","shell.execute_reply.started":"2026-01-13T14:09:08.792207Z","shell.execute_reply":"2026-01-13T14:09:08.838779Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"max_sequence_len = max(len(x) for x in input_sequences)\nprint(f\"Max Sequence Length: {max_sequence_len}\")\ninput_sequences = pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')\nX, y = input_sequences[:, :-1], input_sequences[:, -1]  # Fix slicing\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ny_train = tf.keras.utils.to_categorical(y_train, num_classes=total_words)\ny_test = tf.keras.utils.to_categorical(y_test, num_classes=total_words)\nprint(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T14:09:09.083927Z","iopub.execute_input":"2026-01-13T14:09:09.084481Z","iopub.status.idle":"2026-01-13T14:09:09.203345Z","shell.execute_reply.started":"2026-01-13T14:09:09.084455Z","shell.execute_reply":"2026-01-13T14:09:09.202704Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T14:09:09.411729Z","iopub.execute_input":"2026-01-13T14:09:09.412376Z","iopub.status.idle":"2026-01-13T14:09:09.418198Z","shell.execute_reply.started":"2026-01-13T14:09:09.412353Z","shell.execute_reply":"2026-01-13T14:09:09.417458Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T14:09:57.439039Z","iopub.execute_input":"2026-01-13T14:09:57.439589Z","iopub.status.idle":"2026-01-13T14:09:57.444800Z","shell.execute_reply.started":"2026-01-13T14:09:57.439565Z","shell.execute_reply":"2026-01-13T14:09:57.444135Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the model\nmodel = tf.keras.Sequential()\nmodel.add(tf.keras.layers.Embedding(input_dim=total_words, output_dim=100, input_length=max_sequence_len - 1))\nmodel.add(tf.keras.layers.LSTM(150, return_sequences=False))\nmodel.add(tf.keras.layers.Dense(150, activation='relu'))\nmodel.add(tf.keras.layers.Dense(total_words, activation='softmax'))\nmodel.build(input_shape=(None, max_sequence_len - 1))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\nhistory = model.fit(X_train, y_train, epochs=120, batch_size=64, validation_data=(X_test, y_test))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T14:09:59.299624Z","iopub.execute_input":"2026-01-13T14:09:59.300228Z","iopub.status.idle":"2026-01-13T14:15:33.003440Z","shell.execute_reply.started":"2026-01-13T14:09:59.300205Z","shell.execute_reply":"2026-01-13T14:15:33.002811Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to predict the next word\ndef predict_next_word(model, tokenizer, text, max_sequence_len):\n    token_list = tokenizer.texts_to_sequences([text])[0]\n    if len(token_list) >= max_sequence_len:\n        token_list = token_list[-(max_sequence_len-1):]  # Ensure the sequence length matches\n    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n    predicted = model.predict(token_list, verbose=0)\n    predicted_word_index = np.argmax(predicted, axis=1)\n    for word, index in tokenizer.word_index.items():\n        if index == predicted_word_index:\n            return word\n    return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T14:15:33.005214Z","iopub.execute_input":"2026-01-13T14:15:33.005438Z","iopub.status.idle":"2026-01-13T14:15:33.010425Z","shell.execute_reply.started":"2026-01-13T14:15:33.005421Z","shell.execute_reply":"2026-01-13T14:15:33.009773Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input_text=\"who\"\nprint(f\"input text:{input_text}\")\nmax_sequence_len=model.input_shape[1]+1\nnext_word=predict_next_word(model,tokenizer,input_text,max_sequence_len)\nprint(f\"next word predection:{next_word}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T14:15:33.011215Z","iopub.execute_input":"2026-01-13T14:15:33.011517Z","iopub.status.idle":"2026-01-13T14:15:33.219679Z","shell.execute_reply.started":"2026-01-13T14:15:33.011500Z","shell.execute_reply":"2026-01-13T14:15:33.219037Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save(\"next_word_lstm.h5\")\nimport pickle\nwith open('tokenizer.pickle','wb') as handle:\n    with open ('tokenizer.pickle','wb') as handle:\n        pickle.dump(tokenizer,handle,protocol=pickle.HIGHEST_PROTOCOL)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T14:15:33.221225Z","iopub.execute_input":"2026-01-13T14:15:33.221517Z","iopub.status.idle":"2026-01-13T14:15:33.294322Z","shell.execute_reply.started":"2026-01-13T14:15:33.221500Z","shell.execute_reply":"2026-01-13T14:15:33.293728Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\" more texts \"\"\"\ninput_text=\"after\"\nprint(f\"input text:{input_text}\")\nmax_sequence_len=model.input_shape[1]+1\nnext_word=predict_next_word(model,tokenizer,input_text,max_sequence_len)\nprint(f\"next word predection:{next_word}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T14:15:33.295201Z","iopub.execute_input":"2026-01-13T14:15:33.295484Z","iopub.status.idle":"2026-01-13T14:15:33.366093Z","shell.execute_reply.started":"2026-01-13T14:15:33.295459Z","shell.execute_reply":"2026-01-13T14:15:33.365213Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\" more texts \"\"\"\ninput_text=\"he will go\"\nprint(f\"input text:{input_text}\")\nmax_sequence_len=model.input_shape[1]+1\nnext_word=predict_next_word(model,tokenizer,input_text,max_sequence_len)\nprint(f\"next word predection:{next_word}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T14:37:21.227708Z","iopub.execute_input":"2026-01-13T14:37:21.227970Z","iopub.status.idle":"2026-01-13T14:37:21.296994Z","shell.execute_reply.started":"2026-01-13T14:37:21.227953Z","shell.execute_reply":"2026-01-13T14:37:21.296205Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(6,4))\nplt.plot(history.history['loss'])\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Training Loss\")\nplt.title(\"LSTM Training Loss\")\nplt.grid(True)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T14:34:05.613278Z","iopub.execute_input":"2026-01-13T14:34:05.613591Z","iopub.status.idle":"2026-01-13T14:34:05.829348Z","shell.execute_reply.started":"2026-01-13T14:34:05.613571Z","shell.execute_reply":"2026-01-13T14:34:05.828712Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}